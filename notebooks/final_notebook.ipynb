{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Disclaimer: CURATED DATA IS NOT GITIGNORED TO ALLOW EASE OF MARKING**\n",
    " #### Running the following code cell will create one more needed dataset in data/curated/ that is over 100MB, and therefore could not be uploaded to the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xXDra\\AppData\\Local\\Temp\\ipykernel_65764\\2654570367.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['z_score'] = zscore(df_cleaned['dollar_value'])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"THIS CODE IS FOR THE CURATED DATASET THAT COULD NOT BE UPLOADED TO GITHUB DUE TO SIZE LIMITATION. A COPY OF THE CODE IS AVAILABLE IN ../notebooks/1_preliminary_analysis.ipynb\"\"\"\n",
    "#####################\n",
    "\n",
    "frauddf = pd.read_csv('../data/raw/consumer_fraud_probability.csv')\n",
    "consumerdf = pd.read_csv('../data/raw/tbl_consumer.csv', delimiter='|')\n",
    "userdf = pd.read_parquet('../data/raw/consumer_user_details.parquet')\n",
    "mergeddf1 = consumerdf.merge(userdf, on='consumer_id', how='left')\n",
    "mdf = mergeddf1.merge(frauddf, on='user_id', how='left')\n",
    "df = pd.read_parquet(\"../test\")\n",
    "\n",
    "#mdf.drop(columns=['order_datetime'], inplace=True)\n",
    "\n",
    "mdf['fraud_probability'].fillna(0, inplace=True)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "df_cleaned = df[df['dollar_value'] >= 5]\n",
    "\n",
    "df_cleaned['z_score'] = zscore(df_cleaned['dollar_value'])\n",
    "\n",
    "threshold = np.sqrt(2 * np.log1p(len(df_cleaned)))\n",
    "\n",
    "df_cleaned = df_cleaned[df_cleaned['z_score'].abs() <= threshold]\n",
    "\n",
    "df_cleaned.drop(columns=['z_score'], inplace=True)\n",
    "\n",
    "df_cleaned.head()\n",
    "\n",
    "# merge mdf and df on user_id\n",
    "merged_df = mdf.merge(df, on=['user_id'], how='inner')\n",
    "\n",
    "merged_df.head()\n",
    "\n",
    "# export the merged_df to a parquet file\n",
    "merged_df.to_parquet('../data/curated/consumer_merged.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keeping the API key safe\n",
    "\n",
    "## Setting the Environment Variable in PowerShell\n",
    "In PowerShell, I set the environment variable by running:\n",
    "\n",
    "```powershell\n",
    "$env:API_KEY=\"api_key\"\n",
    "```\n",
    "\n",
    "This stored the API key as an environment variable named `API_KEY`.\n",
    "\n",
    "---\n",
    "\n",
    "## Access the Environment Variable in Python\n",
    "For better security and convenience, I stored the API key in a `.env` file. First, I had to install the `python-dotenv` package:\n",
    "\n",
    "```bash\n",
    "pip install python-dotenv\n",
    "```\n",
    "\n",
    "Then, I created a `.env` file with the following content:\n",
    "\n",
    "```\n",
    "API_KEY=api_key\n",
    "```\n",
    "\n",
    "Finally, I loaded the environment variables from the `.env` file using the following Python code:\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "\n",
    "api_key = os.getenv('API_KEY')\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merchant, Consumer, and Fraud Data Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "The **merchant, consumer, and fraud data** was loaded from both **Parquet** and **CSV files**. The key steps involved include cleaning, transforming, and merging data from multiple sources to derive meaningful insights.\n",
    "\n",
    "### Data Cleaning and Merging Steps\n",
    "\n",
    "1. **Merchant Data**:\n",
    "   - Extracted key information from the `tags` column, splitting it into:\n",
    "     - **Category**\n",
    "     - **Revenue Rate**\n",
    "     - **Take Rate**\n",
    "   - Merged merchant data with **fraud probability** data.\n",
    "   - Missing values in the `fraud_probability` column were filled with **0**.\n",
    "   - Removed duplicate entries.\n",
    "\n",
    "2. **Consumer Data**:\n",
    "   - Merged consumer data with fraud and user details.\n",
    "   - Cleaned the `dollar_value` column by:\n",
    "     - Filtering out **low values**.\n",
    "     - Removing **outliers** using **Z-scores**.\n",
    "\n",
    "### Visualisations\n",
    "\n",
    "#### 1. Revenue Distribution\n",
    "![Revenue Distribution](attachment:7ac04d1c-05fd-47c4-924d-d9163c796733.png)\n",
    "\n",
    "#### 2. Distribution of Log Dollar Value\n",
    "![Distribution of Log Dollar Value](attachment:b474e0f6-adbe-46a0-b14e-2a5cc532805b.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing and Preliminary Analysis of the Data\n",
    "consumer_data = pd.read_parquet('../data/curated/consumer_merged.parquet')\n",
    "merchant_info = pd.read_parquet('../data/curated/merchant_info.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>state</th>\n",
       "      <th>postcode</th>\n",
       "      <th>gender</th>\n",
       "      <th>consumer_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_datetime</th>\n",
       "      <th>fraud_probability</th>\n",
       "      <th>merchant_abn</th>\n",
       "      <th>dollar_value</th>\n",
       "      <th>order_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yolanda Williams</td>\n",
       "      <td>413 Haney Gardens Apt. 742</td>\n",
       "      <td>WA</td>\n",
       "      <td>6935</td>\n",
       "      <td>Female</td>\n",
       "      <td>1195503</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-20</td>\n",
       "      <td>9.805431</td>\n",
       "      <td>80324045558</td>\n",
       "      <td>19.917432</td>\n",
       "      <td>2c7b1606-d9f1-4eac-99f8-97abc979ab41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yolanda Williams</td>\n",
       "      <td>413 Haney Gardens Apt. 742</td>\n",
       "      <td>WA</td>\n",
       "      <td>6935</td>\n",
       "      <td>Female</td>\n",
       "      <td>1195503</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-20</td>\n",
       "      <td>9.805431</td>\n",
       "      <td>62170730910</td>\n",
       "      <td>100.929076</td>\n",
       "      <td>4fe7b08f-72bd-42fb-b2ae-407c3249de55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yolanda Williams</td>\n",
       "      <td>413 Haney Gardens Apt. 742</td>\n",
       "      <td>WA</td>\n",
       "      <td>6935</td>\n",
       "      <td>Female</td>\n",
       "      <td>1195503</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-20</td>\n",
       "      <td>9.805431</td>\n",
       "      <td>32709545238</td>\n",
       "      <td>238.927870</td>\n",
       "      <td>9c5555a5-3e2a-48d3-9b5c-4e54efaf0b67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yolanda Williams</td>\n",
       "      <td>413 Haney Gardens Apt. 742</td>\n",
       "      <td>WA</td>\n",
       "      <td>6935</td>\n",
       "      <td>Female</td>\n",
       "      <td>1195503</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-20</td>\n",
       "      <td>9.805431</td>\n",
       "      <td>74019238521</td>\n",
       "      <td>5.228331</td>\n",
       "      <td>9138e9c7-3c1a-40da-83dc-8bbe3e14e2cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yolanda Williams</td>\n",
       "      <td>413 Haney Gardens Apt. 742</td>\n",
       "      <td>WA</td>\n",
       "      <td>6935</td>\n",
       "      <td>Female</td>\n",
       "      <td>1195503</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-02-20</td>\n",
       "      <td>9.805431</td>\n",
       "      <td>67330176930</td>\n",
       "      <td>144.059911</td>\n",
       "      <td>3f958037-236e-430d-ab86-a1e514dc2584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name                     address state  postcode  gender  \\\n",
       "0  Yolanda Williams  413 Haney Gardens Apt. 742    WA      6935  Female   \n",
       "1  Yolanda Williams  413 Haney Gardens Apt. 742    WA      6935  Female   \n",
       "2  Yolanda Williams  413 Haney Gardens Apt. 742    WA      6935  Female   \n",
       "3  Yolanda Williams  413 Haney Gardens Apt. 742    WA      6935  Female   \n",
       "4  Yolanda Williams  413 Haney Gardens Apt. 742    WA      6935  Female   \n",
       "\n",
       "   consumer_id  user_id order_datetime  fraud_probability  merchant_abn  \\\n",
       "0      1195503        1     2022-02-20           9.805431   80324045558   \n",
       "1      1195503        1     2022-02-20           9.805431   62170730910   \n",
       "2      1195503        1     2022-02-20           9.805431   32709545238   \n",
       "3      1195503        1     2022-02-20           9.805431   74019238521   \n",
       "4      1195503        1     2022-02-20           9.805431   67330176930   \n",
       "\n",
       "   dollar_value                              order_id  \n",
       "0     19.917432  2c7b1606-d9f1-4eac-99f8-97abc979ab41  \n",
       "1    100.929076  4fe7b08f-72bd-42fb-b2ae-407c3249de55  \n",
       "2    238.927870  9c5555a5-3e2a-48d3-9b5c-4e54efaf0b67  \n",
       "3      5.228331  9138e9c7-3c1a-40da-83dc-8bbe3e14e2cc  \n",
       "4    144.059911  3f958037-236e-430d-ab86-a1e514dc2584  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consumer_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merchant_abn</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>revenue_rate</th>\n",
       "      <th>fraud_probability</th>\n",
       "      <th>take_rate_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10023283211</td>\n",
       "      <td>Felis Limited</td>\n",
       "      <td>furniture, home furnishings and equipment shop...</td>\n",
       "      <td>e</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10142254217</td>\n",
       "      <td>Arcu Ac Orci Corporation</td>\n",
       "      <td>cable, satellite, and otHer pay television and...</td>\n",
       "      <td>b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10165489824</td>\n",
       "      <td>Nunc Sed Company</td>\n",
       "      <td>jewelry, watch, clock, and silverware shops</td>\n",
       "      <td>b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10187291046</td>\n",
       "      <td>Ultricies Dignissim Lacus Foundation</td>\n",
       "      <td>wAtch, clock, and jewelry repair shops</td>\n",
       "      <td>b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10192359162</td>\n",
       "      <td>Enim Condimentum PC</td>\n",
       "      <td>music shops - musical instruments, pianos, and...</td>\n",
       "      <td>a</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   merchant_abn                                  name  \\\n",
       "0   10023283211                         Felis Limited   \n",
       "1   10142254217              Arcu Ac Orci Corporation   \n",
       "2   10165489824                      Nunc Sed Company   \n",
       "3   10187291046  Ultricies Dignissim Lacus Foundation   \n",
       "4   10192359162                   Enim Condimentum PC   \n",
       "\n",
       "                                            category revenue_rate  \\\n",
       "0  furniture, home furnishings and equipment shop...            e   \n",
       "1  cable, satellite, and otHer pay television and...            b   \n",
       "2        jewelry, watch, clock, and silverware shops            b   \n",
       "3             wAtch, clock, and jewelry repair shops            b   \n",
       "4  music shops - musical instruments, pianos, and...            a   \n",
       "\n",
       "   fraud_probability  take_rate_value  \n",
       "0                0.0             0.18  \n",
       "1                0.0             4.22  \n",
       "2                0.0             4.40  \n",
       "3                0.0             3.29  \n",
       "4                0.0             6.33  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "Merchant Fraud Probability assesses the risk of a merchant being involved in fraudulent activities. Consumer Fraud Probability estimates the likelihood of customers committing fraud when using the buy now, pay later service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ABS Features\n",
    "\n",
    "We retrieved and processed data from the Australian Bureau of Statistics (ABS), specifically examining the **C21_G02_POA** dataflow. This dataflow contains selected medians and averages for Postal Areas (POA) derived from the 2021 Census. \n",
    "\n",
    "### NOTE - IF this code wants to be run, an API key should be requested. Outputs however, can be seen in 2_absfeatures2.ipynb.\n",
    "\n",
    "## 1. Download Swagger YAML\n",
    "\n",
    "- **Objective**: We started by downloading the Swagger YAML file, which provides a detailed description of the ABS API. This file was essential for understanding how to interact with the API, including available endpoints, request parameters, and response formats.\n",
    "  \n",
    "- **Implementation**:\n",
    "  - A function is called to download the Swagger YAML, which includes error handling to ensure that any issues during the download are caught and reported.\n",
    "  \n",
    "- **Output**: The notebook confirms the successful download of the Swagger YAML, indicating that the necessary documentation for API interaction is now available.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "# URL of the ABS Data API Swagger YAML file, provided by the Australian Bureau of Statistics\n",
    "url = \"https://raw.githubusercontent.com/apigovau/api-descriptions/gh-pages/abs/DataAPI.openapi.yaml\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "    with open(\"../data/raw/swagger.yaml\", \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    print(\"Swagger YAML downloaded successfully!\")\n",
    "\n",
    "except requests.exceptions.HTTPError as http_err:\n",
    "    print(f\"HTTP error occurred: {http_err}\")\n",
    "except Exception as err:\n",
    "    print(f\"An error occurred: {err}\")\n",
    "```\n",
    "\n",
    "## 2. Retrieve Dataflows\n",
    "\n",
    "- **Objective**: Then we retrieved dataflows from the ABS API. Dataflows represent collections of statistical data.\n",
    "  \n",
    "- **Implementation**:\n",
    "  - We make an API call to the ABS endpoint to fetch dataflows in XML format.\n",
    "  - The XML response is parsed to extract relevant information about the dataflows.\n",
    "\n",
    "- **Output**: The successful retrieval of dataflows is confirmed, and the data is saved to a specified location for later use. Now we can access detailed information about the datasets available in the ABS API.\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "api_key = os.getenv('API_KEY')\n",
    "\n",
    "# API endpoint\n",
    "url = \"https://api.data.abs.gov.au/dataflow/ABS\"  # Replace 'ABS' with the agency ID if needed\n",
    "\n",
    "# API Key (if required)\n",
    "headers = {\n",
    "    \"x-api-key\": api_key, #api_key  \n",
    "    \"Accept\": \"application/xml\"  # Specify that we want XML format\n",
    "}\n",
    "\n",
    "# Make the API call\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check the response status\n",
    "if response.status_code == 200:\n",
    "    xml_data = response.text  # Get the XML response as text\n",
    "    print(\"Dataflows retrieved successfully in XML format!\")\n",
    "    with open(\"../data/raw/dataflows.xml\", \"w\") as file:\n",
    "        file.write(xml_data)\n",
    "    print(\"Dataflows saved successfully to ../temp/dataflows.xml\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve dataflows. Status code: {response.status_code}\")\n",
    "    print(response.text)\n",
    "\n",
    "```\n",
    "\n",
    "## 3. Extract Dataflow Information\n",
    "\n",
    "- **Objective**: Then we extracted specific information about the **C21_G02_POA** dataflow. The 2021 Census data provided the most recent and comprehensive demographic information available. Utilizing datasets from the same time period (2021) reduces discrepancies that can arise when analysing older data alongside newer datasets. **We chose to request by the POA (Postal Area) as the given data only had postcodes as identifying information**.\n",
    "  \n",
    "- **Implementation**:\n",
    "  - We search through the parsed XML data to find the relevant dataflow by its ID, **C21_G02_POA**.\n",
    "  - Key attributes are extracted, including:\n",
    "    - **ID**: The identifier for the dataflow.\n",
    "    - **Agency ID**: The agency responsible for the data (ABS).\n",
    "    - **Version**: The version of the dataflow, indicating updates or changes.\n",
    "    - **Is Final**: A flag indicating whether the dataflow is finalized and considered reliable for use.\n",
    "    - **Name**: A descriptive title of the dataflow.\n",
    "    - **Description**: A comprehensive description detailing what the dataflow includes and how it should be interpreted.\n",
    "\n",
    "```python\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Load and parse the XML file\n",
    "xml_file_path = '../data/raw/dataflows.xml'\n",
    "tree = ET.parse(xml_file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Adjust the namespace to include the common prefix for data extraction\n",
    "namespace = {\n",
    "    'sdmx': 'http://www.sdmx.org/resources/sdmxml/sdmx-2.1',\n",
    "    'structure': 'http://www.sdmx.org/resources/sdmxml/schemas/v2_1/structure',\n",
    "    'common': 'http://www.sdmx.org/resources/sdmxml/schemas/v2_1/common'\n",
    "}\n",
    "\n",
    "# Attempt to retrieve C21_G02_POA dataflow details again\n",
    "dataflow_info_poa = None\n",
    "\n",
    "# Search through the Dataflows to find C21_G02_POA\n",
    "for dataflow in root.findall('.//structure:Dataflow', namespace):\n",
    "    if dataflow.get('id') == 'C21_G02_POA':\n",
    "        dataflow_info_poa = {\n",
    "            'id': dataflow.get('id'),\n",
    "            'agencyID': dataflow.get('agencyID'),\n",
    "            'version': dataflow.get('version'),\n",
    "            'isFinal': dataflow.get('isFinal'),\n",
    "            'name': dataflow.find('common:Name', namespace).text if dataflow.find('common:Name', namespace) is not None else None,\n",
    "            'description': dataflow.find('common:Description', namespace).text if dataflow.find('common:Description', namespace) is not None else None,\n",
    "        }\n",
    "        break\n",
    "\n",
    "dataflow_info_poa\n",
    "```\n",
    "\n",
    "## 4. Retrieving the data, and cleaning\n",
    "\n",
    "- **Objective**: We then retrieved the data with an API call, and then saved. The CSV originally was in a **very ugly format**. **Since the ABS Data API is still in Beta**, the dataset was not pivoted. Each feature was originally kept in a column called \"MEDAVG\", and was manually pivoted to match up with the actual name of each feature.\n",
    "\n",
    "```python\n",
    "# API base URL\n",
    "base_url = \"https://api.data.abs.gov.au/data\"\n",
    "\n",
    "\n",
    "# Function to make API call to retrieve data in CSV format\n",
    "def get_data_csv(dataflow_id, data_key, params=None):\n",
    "    url = f\"{base_url}/{dataflow_id}/{data_key}?format=csv\"  # Specify CSV format in the URL\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.text  # Return the CSV response as text\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {dataflow_id}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Retrieve all data for C21_G02_POA\n",
    "dataflow_id = \"C21_G02_POA\"  # Dataflow ID for C21_G02_POA\n",
    "data_key = \"all\"  # Use \"all\" to retrieve all data\n",
    "\n",
    "# Get the data\n",
    "c21_g02_poa_csv_result = get_data_csv(dataflow_id, data_key)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "if c21_g02_poa_csv_result:\n",
    "    with open(\"../data/raw/C21_G02_POA_data.csv\", \"w\") as f:\n",
    "        f.write(c21_g02_poa_csv_result)\n",
    "    print(\"C21_G02_POA data retrieved and saved successfully to 'C21_G02_POA_data.csv'.\")\n",
    "\n",
    "```\n",
    "\n",
    "## 5. Dropping columns\n",
    "\n",
    "- **Objective**: We then removed unneeded columns from the data. Only Median_age_persons, Median_total_household_income_weekly\tMedian_mortgage_repay_monthly, and Median_rent_weekly was left. Then it was saved in curated data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POA_CODE_2021</th>\n",
       "      <th>Median_age_persons</th>\n",
       "      <th>Median_mortgage_repay_monthly</th>\n",
       "      <th>Median_rent_weekly</th>\n",
       "      <th>Median_tot_hhd_inc_weekly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>32</td>\n",
       "      <td>2800</td>\n",
       "      <td>625</td>\n",
       "      <td>2225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007</td>\n",
       "      <td>30</td>\n",
       "      <td>2500</td>\n",
       "      <td>500</td>\n",
       "      <td>1805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008</td>\n",
       "      <td>28</td>\n",
       "      <td>2600</td>\n",
       "      <td>525</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009</td>\n",
       "      <td>37</td>\n",
       "      <td>2800</td>\n",
       "      <td>580</td>\n",
       "      <td>2422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010</td>\n",
       "      <td>36</td>\n",
       "      <td>2900</td>\n",
       "      <td>550</td>\n",
       "      <td>2297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   POA_CODE_2021  Median_age_persons  Median_mortgage_repay_monthly  \\\n",
       "0           2000                  32                           2800   \n",
       "1           2007                  30                           2500   \n",
       "2           2008                  28                           2600   \n",
       "3           2009                  37                           2800   \n",
       "4           2010                  36                           2900   \n",
       "\n",
       "   Median_rent_weekly  Median_tot_hhd_inc_weekly  \n",
       "0                 625                       2225  \n",
       "1                 500                       1805  \n",
       "2                 525                       1746  \n",
       "3                 580                       2422  \n",
       "4                 550                       2297  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs_data = pd.read_csv('../data/curated/2021Census_G02_AUST_POA_curated.csv')\n",
    "abs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "One significant challenge in this project was the mismatch in the date ranges between the primary dataset and the ABS dataset. The primary dataset spanned from April 2021 to October 2022, while the ABS dataset ended in August 2021 which is the most current data available. This discrepancy resulted in incomplete data coverage for the earlier months of the primary dataset (April to July 2021), where no corresponding ABS data was available. \n",
    "Moreover, the primary dataset included certain postcodes that are either unused or associated with postboxes, which were not present in the ABS dataset. This mismatch led to missing values during the merging process, potentially affecting the accuracy of eographical or postcode-based analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The merchants' data was categorised based on keywords found in their category descriptions. A list of potential categories was established, including 'Retail Goods', 'Media and Technology', 'Furniture and Home Furnishing', 'Services', and 'Automotives'. Natural Language Processing techniques were applied by normalising the category text and filtering out common stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merchant_abn</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>revenue_rate</th>\n",
       "      <th>fraud_probability</th>\n",
       "      <th>take_rate_value</th>\n",
       "      <th>category_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10023283211</td>\n",
       "      <td>Felis Limited</td>\n",
       "      <td>furniture, home furnishings and equipment shop...</td>\n",
       "      <td>e</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>Furniture and Home Furnishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10142254217</td>\n",
       "      <td>Arcu Ac Orci Corporation</td>\n",
       "      <td>cable, satellite, and otHer pay television and...</td>\n",
       "      <td>b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.22</td>\n",
       "      <td>Media and Technology, Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10165489824</td>\n",
       "      <td>Nunc Sed Company</td>\n",
       "      <td>jewelry, watch, clock, and silverware shops</td>\n",
       "      <td>b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.40</td>\n",
       "      <td>Retail Goods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10187291046</td>\n",
       "      <td>Ultricies Dignissim Lacus Foundation</td>\n",
       "      <td>wAtch, clock, and jewelry repair shops</td>\n",
       "      <td>b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.29</td>\n",
       "      <td>Retail Goods, Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10192359162</td>\n",
       "      <td>Enim Condimentum PC</td>\n",
       "      <td>music shops - musical instruments, pianos, and...</td>\n",
       "      <td>a</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.33</td>\n",
       "      <td>Retail Goods</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   merchant_abn                                  name  \\\n",
       "0   10023283211                         Felis Limited   \n",
       "1   10142254217              Arcu Ac Orci Corporation   \n",
       "2   10165489824                      Nunc Sed Company   \n",
       "3   10187291046  Ultricies Dignissim Lacus Foundation   \n",
       "4   10192359162                   Enim Condimentum PC   \n",
       "\n",
       "                                            category revenue_rate  \\\n",
       "0  furniture, home furnishings and equipment shop...            e   \n",
       "1  cable, satellite, and otHer pay television and...            b   \n",
       "2        jewelry, watch, clock, and silverware shops            b   \n",
       "3             wAtch, clock, and jewelry repair shops            b   \n",
       "4  music shops - musical instruments, pianos, and...            a   \n",
       "\n",
       "   fraud_probability  take_rate_value                  category_label  \n",
       "0                0.0             0.18   Furniture and Home Furnishing  \n",
       "1                0.0             4.22  Media and Technology, Services  \n",
       "2                0.0             4.40                    Retail Goods  \n",
       "3                0.0             3.29          Retail Goods, Services  \n",
       "4                0.0             6.33                    Retail Goods  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant_categories = pd.read_parquet('../data/curated/merchant_categories.parquet')\n",
    "merchant_categories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transaction data was grouped by each unique combination of merchant abn and datetime, the dollar value and number of transactions were then aggregated to find the total sale value and transaction number of each merchant by date. This was then combined with the merchant category data on merchant abn, to match each merchant with their respective category. The mean of dollar value and number of transactions were taken, grouped by merchant abn. This was then merged with merchant fraud data and null values were replaced with 0 (since we assumed the merchants who were not in the merchant fraud data were not flagged for the potential to be fraud, thus had no chance of fraud). Categories were one hot encoded for each category separated by commas. A model was then trained on this dataset. The model can be used to predict instances with missing revenue.\n",
    "\n",
    "Gradient Boosting was chosen, as it had the highest R Squared score, a train-split was done on the dataset, and was then used to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>take_rate_value</th>\n",
       "      <th>fraud_probability</th>\n",
       "      <th>average_monthly_revenue</th>\n",
       "      <th>average_monthly_transactions</th>\n",
       "      <th>Retail Goods</th>\n",
       "      <th>Media and Technology</th>\n",
       "      <th>Furniture and Home Furnishing</th>\n",
       "      <th>Services</th>\n",
       "      <th>Automotives</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216.736529</td>\n",
       "      <td>155.285714</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.716001</td>\n",
       "      <td>144.571429</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11236.094771</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>3.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>119.526004</td>\n",
       "      <td>16.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>6.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>462.329450</td>\n",
       "      <td>19.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    take_rate_value  fraud_probability  average_monthly_revenue  \\\n",
       "0              0.18                0.0               216.736529   \n",
       "21             4.22                0.0                37.716001   \n",
       "42             4.40                0.0             11236.094771   \n",
       "53             3.29                0.0               119.526004   \n",
       "73             6.33                0.0               462.329450   \n",
       "\n",
       "    average_monthly_transactions  Retail Goods  Media and Technology  \\\n",
       "0                     155.285714             0                     0   \n",
       "21                    144.571429             0                     1   \n",
       "42                      0.454545             1                     0   \n",
       "53                     16.800000             1                     0   \n",
       "73                     19.250000             1                     0   \n",
       "\n",
       "    Furniture and Home Furnishing  Services  Automotives  \n",
       "0                               1         0            0  \n",
       "21                              0         0            0  \n",
       "42                              0         0            0  \n",
       "53                              0         0            0  \n",
       "73                              0         0            0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merchant_revenue_data = pd.read_parquet('../data/curated/merged_merchant_info_with_categories_and_features.parquet')\n",
    "merchant_revenue_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 5972074.430302302\n",
      "Accuracy (R^2 score): 0.43355656146416444\n",
      "Feature Importances: [0.13298536 0.12425103 0.6537074  0.02590308 0.01044799 0.04256255\n",
      " 0.00681037 0.00333221]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y = merchant_revenue_data['average_monthly_revenue']\n",
    "\n",
    "X = merchant_revenue_data.drop(columns=['average_monthly_revenue'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
    "\n",
    "model = GradientBoostingRegressor(\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=10,\n",
    "    n_estimators=300\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(\"Accuracy (R^2 score):\", accuracy)\n",
    "\n",
    "print(\"Feature Importances:\", model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict consumer fraud, transaction data was merged with fraud probability information. First, the transaction and fraud data are combined, creating two classification datasets. In merged_fraud_15, transactions with a fraud probability of 15% or less are classified as non-fraud (0), while in merged_fraud_20, the threshold is set to 20% or less. Logistic regression models are trained on both datasets to classify whether a transaction is fraudulent. Separately, the original merged_fraud dataset, which contains actual fraud probabilities, is used for linear regression to predict fraud probability directly. These models can be used to predict consumer fraud for future consumer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/curated/merged_fraud_1.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m merged_fraud_15 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/curated/merged_fraud_1.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m merged_fraud_20 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/curated/merged_fraud_2.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m merged_fraud \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/curated/merged_fraud.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    668\u001b[0m     path,\n\u001b[1;32m    669\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[1;32m    670\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[1;32m    671\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    672\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[1;32m    673\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    674\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    676\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parquet.py:267\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    265\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    268\u001b[0m     path,\n\u001b[1;32m    269\u001b[0m     filesystem,\n\u001b[1;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    275\u001b[0m         path_or_handle,\n\u001b[1;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/parquet.py:140\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    130\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 140\u001b[0m     handles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m    141\u001b[0m         path_or_handle, mode, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    143\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/curated/merged_fraud_1.parquet'"
     ]
    }
   ],
   "source": [
    "merged_fraud_15 = pd.read_parquet('../data/curated/merged_fraud_1.parquet')\n",
    "merged_fraud_20 = pd.read_parquet('../data/curated/merged_fraud_2.parquet')\n",
    "merged_fraud = pd.read_parquet('../data/curated/merged_fraud.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_fraud_15.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_fraud_20.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_fraud.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify merchants with the highest Compound Weekly Growth Rate (CWGR), the weekly revenue was analysed. After loading and aggregating transaction data by merchant and week, Prophet models were trained for each merchant to forecast future revenue and compute CWGR. Merchants were ranked based on their CWGR, and the top 10 were selected for further analysis. The revenue of these top merchants was scaled using MinMax scaling, and smooth curves were generated to visualise the trends in their monthly revenue over time. This analysis helped identify merchants with the fastest revenue growth, which could signal increased profitability and potential for the BNPL firm. However, it is important to assess whether the growth is sustainable or driven by temporary factors such as seasonal sales (e.g., during Christmas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting with Prophet: Mathematical Foundation and Model Selection\n",
    "\n",
    "## Why a Piecewise Linear Model Suited Our Case\n",
    "\n",
    "Using a piecewise linear model in Prophet proved particularly advantageous for our analysis of merchant revenue data due to several key factors:\n",
    "\n",
    "1. **Capturing Trend Changes**:\n",
    "   - **Adaptability**: A piecewise linear model allowed for shifts in the trend direction at specified changepoints, making it useful where merchant transaction behaviours changed over time due to market trends or seasonal demand.\n",
    "   - **Flexible Growth Rates**: The model could accommodate varying growth rates, allowing for a more accurate representation of trends.\n",
    "\n",
    "2. **Handling Seasonal Patterns**:\n",
    "   - **Combined Seasonal and Trend Modeling**: Prophet’s ability to model both seasonality and long-term trends was invaluable, effectively capturing fluctuations that occurred at specific times of the year.\n",
    "   - **Clear Interpretation**: The ability to see where trends changed provided clearer insights into the timing and impact of different business strategies or external events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Linear Growth Was Chosen Over Logistic Growth\n",
    "\n",
    "Choosing linear growth over logistic growth in Prophet for forecasting merchant transaction data stemmed from several key factors:\n",
    "\n",
    "1. **Nature of the Data**:\n",
    "   - **Current Market Conditions**: Linear growth reflected scenarios where demand was steadily increasing without constraints, as only a short time frame (13 weeks) was projected.\n",
    "\n",
    "2. **Simplicity and Interpretability**:\n",
    "   - **Simplicity**: Linear models were easier to implement and interpret, providing a straightforward relationship between time and transaction values.\n",
    "   - **Clear Forecasting**: We could easily project future values based on past trends.\n",
    "\n",
    "3. **Flexibility with Changepoints**:\n",
    "   - **Piecewise Linear Growth**: The model accommodated multiple growth phases through changepoints, capturing significant shifts in growth without the constraints of logistic growth.\n",
    "   - **Avoiding Overfitting**: Logistic models could overfit the data if capacity was set inaccurately, while linear models avoided this risk.\n",
    "\n",
    "4. **Data Characteristics**:\n",
    "   - **Long-Term Trends**: Our historical data showed consistent upward trends without saturation, which aligned well with a linear model.\n",
    "   - **Early Growth Stages**: In the initial stages of the business or product, growth was often more linear as the market was being penetrated and customer adoption increased.\n",
    "\n",
    "5. **Less Complexity in Forecasting**:\n",
    "   - **Reduced Parameter Complexity**: Logistic growth models required additional parameters that needed accurate estimation, whereas linear growth simplified this process.\n",
    "\n",
    "### Performance and Scalability of Prophet\n",
    "\n",
    "- **Speed**: Prophet is efficient, allowing it to quickly generate forecasts even with large datasets. This was particularly beneficial as it could process and produce results in a fraction of the time compared to more complex models.\n",
    "\n",
    "- **Training on Multiple Merchants**: Prophet effectively handled forecasting for a large number of individual merchants, a bit more than 4,000. Each merchant could be modeled independently, making it feasible to scale the forecasting process without a significant increase in computational resources.\n",
    "\n",
    "- **Scalability**: This model is scalable. It could handle varying amounts of data without significant reconfiguration. If new merchants were added or historical data expanded, our model could adapt to these changes efficiently. Its ability to run independently for each merchant allowed us to maintain performance even as our datasets grew.\n",
    "\n",
    "## Additive Model Characteristics\n",
    "\n",
    "1. **Additive Components**: \n",
    "   Prophet models time series data as a sum of trend, seasonality, holiday effects, and noise:\n",
    "   \\[\n",
    "   y(t) = g(t) + s(t) + h(t) + \\epsilon_t\n",
    "   \\]\n",
    "   Each of these components is calculated based on the historical data of the individual time series. This was good in our case as **we could clearly see holiday effects during Christmas**. \n",
    "\n",
    "## Our two Prediction Approaches\n",
    "\n",
    "1. **Individual Predictions for Each Merchant (For generating CWGR)**:\n",
    "   - When we forecast each merchant's sales individually using Prophet, the model estimates its unique trend and seasonal patterns based on that merchant's historical data. Each merchant can have different seasonal effects, growth rates, and responses to holidays.\n",
    "   - After making predictions for each merchant, we can sum the predicted values for the desired future time periods.\n",
    "\n",
    "2. **Aggregating Data Before Prediction (For Visualisation)**: \n",
    "   - If we combine the historical transaction data of all merchants into a single time series and then apply Prophet, the model will fit a single trend and seasonal pattern to the aggregated data.\n",
    "   - This approach assumes that all merchants share similar seasonal patterns and trends, which is the case for the top 100 and bottom 100 merchants for visualisation purposes. And therefore the aggregated model captures common features across all merchants. Due to this aggregation, the line looks largely straight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Compound Weekly Growth Rate (CWGR) in Merchant Revenue Forecasting\n",
    "\n",
    "## CWGR (feature engineering)\n",
    "\n",
    "The **Compound Weekly Growth Rate (CWGR)** was used to measure the growth of merchant revenue over time. It quantifies how much revenue is expected to grow week over week. By analysing CWGR, we can assess how effectively a merchant is increasing their revenue, making it the perfect metric for evaluating growth trajectories.\n",
    "\n",
    "## Calculating CWGR from Predictions\n",
    "\n",
    "In the context of forecasting, CWGR can be derived from predictions made for the next 13 weeks, typically representing the upcoming quarter. By using a forecasting model like Prophet, we can estimate expected revenue for each week over this time frame. The CWGR was then calculated using the formula:\n",
    "\n",
    "CWGR = (Final Revenue / Initial Revenue)^(1/n) - 1\n",
    "\n",
    "Where:\n",
    "- **Final Revenue** is the predicted revenue at the end of the 13 weeks.\n",
    "- **Initial Revenue** is the predicted revenue at the beginning of the period.\n",
    "- **n** is the number of weeks (in this case, 13).\n",
    "\n",
    "## Advantages\n",
    "\n",
    "1. **Timely Insights**:\n",
    "   - The CWGR calculated from a short time frame, such as the next quarter, allowed us to quickly gauge the effectiveness of merchants' different strategies.\n",
    "\n",
    "2. **Short-Term Forecasting Reliability**:\n",
    "   - Since it is short time frame, predictions for the next 13 weeks provided a reliable indicator of growth.\n",
    "\n",
    "## Impact on Final Merchant Ranking\n",
    "\n",
    "The CWGR emerged as a significant feature in our final merchant ranking. By incorporating CWGR into the ranking criteria, we could prioritise merchants based on their predicted growth potential rather than just their historical performance. This provided a more dynamic view of merchant success and allowed us to recognise those poised for future growth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Set order_datetime as the index\n",
    "df.set_index('order_datetime', inplace=True)\n",
    "\n",
    "# Aggregate dollar_value for each merchant for each month\n",
    "weekly_aggregation = df.groupby('merchant_abn').resample('W').sum()\n",
    "\n",
    "weekly_aggregation.drop(columns=['merchant_abn'], inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Initialize a dictionary to store the Prophet models and CWGR for each merchant\n",
    "merchant_models = {}\n",
    "merchant_cwgr = {}\n",
    "\n",
    "# Iterate through each merchant's data in merchant_dfs\n",
    "for merchant_data in merchant_dfs:\n",
    "    for merchant_abn, df in merchant_data.items():\n",
    "        # Prepare the data for Prophet\n",
    "        df.rename(columns={'order_datetime': 'ds', 'dollar_value': 'y'}, inplace=True)\n",
    "        \n",
    "        # Check if the DataFrame has less than or equal to 1 entry\n",
    "        if len(df) <= 1:\n",
    "            print(f\"Skipping Merchant {merchant_abn} due to insufficient data.\")\n",
    "            merchant_cwgr[merchant_abn] = 0\n",
    "            continue\n",
    "\n",
    "        # Initialize and train the Prophet model\n",
    "        model = Prophet()\n",
    "        print(f\"Training model for Merchant {merchant_abn}...\")\n",
    "        model.fit(df)\n",
    "        \n",
    "        # Store the trained model\n",
    "        merchant_models[merchant_abn] = model\n",
    "        \n",
    "        # Make future dataframe for prediction\n",
    "        future = model.make_future_dataframe(periods=13, freq='W')\n",
    "        forecast = model.predict(future)\n",
    "        \n",
    "        # Calculate CWGR\n",
    "        initial_value = df['y'].iloc[0]\n",
    "        final_value = forecast['yhat'].iloc[-1]\n",
    "        weeks = len(forecast)\n",
    "        cwgr = ((final_value / initial_value) ** (1 / weeks)) - 1\n",
    "        \n",
    "        # Store the CWGR\n",
    "        merchant_cwgr[merchant_abn] = cwgr\n",
    "\n",
    "merchant_cwgr\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
